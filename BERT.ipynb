{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fcd9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from transformers import BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f7fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('processed_lyrics.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dbd5586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALink</th>\n",
       "      <th>SName</th>\n",
       "      <th>SLink</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Idiom</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/12-stones/</td>\n",
       "      <td>World So Cold</td>\n",
       "      <td>/12-stones/world-so-cold.html</td>\n",
       "      <td>It starts with pain, followed by hate. Fueled ...</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Rock</td>\n",
       "      <td>12 Stones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/12-stones/</td>\n",
       "      <td>Broken</td>\n",
       "      <td>/12-stones/broken.html</td>\n",
       "      <td>Freedom!. Alone again again alone. Patiently w...</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Rock</td>\n",
       "      <td>12 Stones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/12-stones/</td>\n",
       "      <td>3 Leaf Loser</td>\n",
       "      <td>/12-stones/3-leaf-loser.html</td>\n",
       "      <td>Biting the hand that feeds you, lying to the v...</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Rock</td>\n",
       "      <td>12 Stones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/12-stones/</td>\n",
       "      <td>Anthem For The Underdog</td>\n",
       "      <td>/12-stones/anthem-for-the-underdog.html</td>\n",
       "      <td>You say you know just who I am. But you can't ...</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Rock</td>\n",
       "      <td>12 Stones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/12-stones/</td>\n",
       "      <td>Adrenaline</td>\n",
       "      <td>/12-stones/adrenaline.html</td>\n",
       "      <td>My heart is beating faster can't control these...</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Rock</td>\n",
       "      <td>12 Stones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94809</th>\n",
       "      <td>/zendaya/</td>\n",
       "      <td>Too Much</td>\n",
       "      <td>/zendaya/too-much.html</td>\n",
       "      <td>Baby we're too much, much to handle. Understan...</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Zendaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94810</th>\n",
       "      <td>/zendaya/</td>\n",
       "      <td>Watch Me (feat. Bella Thorne)</td>\n",
       "      <td>/zendaya/watch-me-feat-bella-thorne.html</td>\n",
       "      <td>I don't need no one to tell me how to feel. Th...</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Zendaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94811</th>\n",
       "      <td>/zendaya/</td>\n",
       "      <td>White Christmas</td>\n",
       "      <td>/zendaya/white-christmas.html</td>\n",
       "      <td>I'm, dreaming of a white, Christmas. Just like...</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Zendaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94812</th>\n",
       "      <td>/zendaya/</td>\n",
       "      <td>Whodunit</td>\n",
       "      <td>/zendaya/whodunit.html</td>\n",
       "      <td>Ha, a-plus on the track. Ha, ha, ha, uh, yeah....</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Zendaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94813</th>\n",
       "      <td>/zendaya/</td>\n",
       "      <td>Wonderful Life</td>\n",
       "      <td>/zendaya/wonderful-life.html</td>\n",
       "      <td>Take a look around. And see the world we think...</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Zendaya</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94814 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ALink                          SName  \\\n",
       "0      /12-stones/                  World So Cold   \n",
       "1      /12-stones/                         Broken   \n",
       "2      /12-stones/                   3 Leaf Loser   \n",
       "3      /12-stones/        Anthem For The Underdog   \n",
       "4      /12-stones/                     Adrenaline   \n",
       "...            ...                            ...   \n",
       "94809    /zendaya/                       Too Much   \n",
       "94810    /zendaya/  Watch Me (feat. Bella Thorne)   \n",
       "94811    /zendaya/                White Christmas   \n",
       "94812    /zendaya/                       Whodunit   \n",
       "94813    /zendaya/                 Wonderful Life   \n",
       "\n",
       "                                          SLink  \\\n",
       "0                 /12-stones/world-so-cold.html   \n",
       "1                        /12-stones/broken.html   \n",
       "2                  /12-stones/3-leaf-loser.html   \n",
       "3       /12-stones/anthem-for-the-underdog.html   \n",
       "4                    /12-stones/adrenaline.html   \n",
       "...                                         ...   \n",
       "94809                    /zendaya/too-much.html   \n",
       "94810  /zendaya/watch-me-feat-bella-thorne.html   \n",
       "94811             /zendaya/white-christmas.html   \n",
       "94812                    /zendaya/whodunit.html   \n",
       "94813              /zendaya/wonderful-life.html   \n",
       "\n",
       "                                                   Lyric    Idiom Genre  \\\n",
       "0      It starts with pain, followed by hate. Fueled ...  ENGLISH  Rock   \n",
       "1      Freedom!. Alone again again alone. Patiently w...  ENGLISH  Rock   \n",
       "2      Biting the hand that feeds you, lying to the v...  ENGLISH  Rock   \n",
       "3      You say you know just who I am. But you can't ...  ENGLISH  Rock   \n",
       "4      My heart is beating faster can't control these...  ENGLISH  Rock   \n",
       "...                                                  ...      ...   ...   \n",
       "94809  Baby we're too much, much to handle. Understan...  ENGLISH   Pop   \n",
       "94810  I don't need no one to tell me how to feel. Th...  ENGLISH   Pop   \n",
       "94811  I'm, dreaming of a white, Christmas. Just like...  ENGLISH   Pop   \n",
       "94812  Ha, a-plus on the track. Ha, ha, ha, uh, yeah....  ENGLISH   Pop   \n",
       "94813  Take a look around. And see the world we think...  ENGLISH   Pop   \n",
       "\n",
       "          Artist  \n",
       "0      12 Stones  \n",
       "1      12 Stones  \n",
       "2      12 Stones  \n",
       "3      12 Stones  \n",
       "4      12 Stones  \n",
       "...          ...  \n",
       "94809    Zendaya  \n",
       "94810    Zendaya  \n",
       "94811    Zendaya  \n",
       "94812    Zendaya  \n",
       "94813    Zendaya  \n",
       "\n",
       "[94814 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83e2fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df.sample(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26ec6729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALink</th>\n",
       "      <th>SName</th>\n",
       "      <th>SLink</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Idiom</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75663</th>\n",
       "      <td>/david-archuleta/</td>\n",
       "      <td>Shine a Light</td>\n",
       "      <td>/david-archuleta/shine-a-light.html</td>\n",
       "      <td>There is a beacon. You might not see the light...</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Pop</td>\n",
       "      <td>David Archuleta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45819</th>\n",
       "      <td>/the-go-team/</td>\n",
       "      <td>Doing It Right</td>\n",
       "      <td>/the-go-team/doing-it-right.html</td>\n",
       "      <td>Do it! Do it! All right!. Do it! Do it! All ri...</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Rock</td>\n",
       "      <td>The Go! Team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21810</th>\n",
       "      <td>/killing-joke/</td>\n",
       "      <td>Absent Friends</td>\n",
       "      <td>/killing-joke/absent-friends.html</td>\n",
       "      <td>I remember the sun beating down. Across my bac...</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Killing Joke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70872</th>\n",
       "      <td>/bee-gees/</td>\n",
       "      <td>Woman In Love</td>\n",
       "      <td>/bee-gees/woman-in-love.html</td>\n",
       "      <td>Life is a moment in space. When the dream is g...</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Bee Gees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84756</th>\n",
       "      <td>/madonna/</td>\n",
       "      <td>Live To Tell</td>\n",
       "      <td>/madonna/live-to-tell.html</td>\n",
       "      <td>I have a tale to tell.. Sometimes it gets so h...</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Madonna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64416</th>\n",
       "      <td>/snoop-dogg/</td>\n",
       "      <td>Speaker</td>\n",
       "      <td>/snoop-dogg/speaker.html</td>\n",
       "      <td>[Intro]. Snoop Dogg. Konvict Music, Akon and D...</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Hip Hop</td>\n",
       "      <td>Snoop Dogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85770</th>\n",
       "      <td>/michael-jackson/</td>\n",
       "      <td>Seeing Voices</td>\n",
       "      <td>/michael-jackson/seeing-voices.html</td>\n",
       "      <td>I sing of what you see and might think about. ...</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Michael Jackson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71975</th>\n",
       "      <td>/bread/</td>\n",
       "      <td>Diary</td>\n",
       "      <td>/bread/diary.html</td>\n",
       "      <td>I found her diary underneath a tree. and start...</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Bread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75732</th>\n",
       "      <td>/david-guetta/</td>\n",
       "      <td>Bang My Head (feat. Sia &amp; Fetty Wap) (Club Kil...</td>\n",
       "      <td>/david-guetta/bang-my-head-feat-sia-fetty-wap-...</td>\n",
       "      <td>I was bound, was tired. Hadn't seen a light so...</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Pop</td>\n",
       "      <td>David Guetta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77169</th>\n",
       "      <td>/fifth-harmony/</td>\n",
       "      <td>Down (Feat. Gucci Mane)</td>\n",
       "      <td>/fifth-harmony/down-feat-gucci-mane.html</td>\n",
       "      <td>It's Gucci. Fifth Harmony. I need somebody wit...</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Fifth Harmony</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ALink                                              SName  \\\n",
       "75663  /david-archuleta/                                      Shine a Light   \n",
       "45819      /the-go-team/                                     Doing It Right   \n",
       "21810     /killing-joke/                                     Absent Friends   \n",
       "70872         /bee-gees/                                      Woman In Love   \n",
       "84756          /madonna/                                       Live To Tell   \n",
       "...                  ...                                                ...   \n",
       "64416       /snoop-dogg/                                            Speaker   \n",
       "85770  /michael-jackson/                                      Seeing Voices   \n",
       "71975            /bread/                                              Diary   \n",
       "75732     /david-guetta/  Bang My Head (feat. Sia & Fetty Wap) (Club Kil...   \n",
       "77169    /fifth-harmony/                            Down (Feat. Gucci Mane)   \n",
       "\n",
       "                                                   SLink  \\\n",
       "75663                /david-archuleta/shine-a-light.html   \n",
       "45819                   /the-go-team/doing-it-right.html   \n",
       "21810                  /killing-joke/absent-friends.html   \n",
       "70872                       /bee-gees/woman-in-love.html   \n",
       "84756                         /madonna/live-to-tell.html   \n",
       "...                                                  ...   \n",
       "64416                           /snoop-dogg/speaker.html   \n",
       "85770                /michael-jackson/seeing-voices.html   \n",
       "71975                                  /bread/diary.html   \n",
       "75732  /david-guetta/bang-my-head-feat-sia-fetty-wap-...   \n",
       "77169           /fifth-harmony/down-feat-gucci-mane.html   \n",
       "\n",
       "                                                   Lyric    Idiom    Genre  \\\n",
       "75663  There is a beacon. You might not see the light...  ENGLISH      Pop   \n",
       "45819  Do it! Do it! All right!. Do it! Do it! All ri...  ENGLISH     Rock   \n",
       "21810  I remember the sun beating down. Across my bac...  ENGLISH     Rock   \n",
       "70872  Life is a moment in space. When the dream is g...  ENGLISH      Pop   \n",
       "84756  I have a tale to tell.. Sometimes it gets so h...  ENGLISH      Pop   \n",
       "...                                                  ...      ...      ...   \n",
       "64416  [Intro]. Snoop Dogg. Konvict Music, Akon and D...  ENGLISH  Hip Hop   \n",
       "85770  I sing of what you see and might think about. ...  ENGLISH      Pop   \n",
       "71975  I found her diary underneath a tree. and start...  ENGLISH      Pop   \n",
       "75732  I was bound, was tired. Hadn't seen a light so...  ENGLISH      Pop   \n",
       "77169  It's Gucci. Fifth Harmony. I need somebody wit...  ENGLISH      Pop   \n",
       "\n",
       "                Artist  \n",
       "75663  David Archuleta  \n",
       "45819     The Go! Team  \n",
       "21810     Killing Joke  \n",
       "70872         Bee Gees  \n",
       "84756          Madonna  \n",
       "...                ...  \n",
       "64416       Snoop Dogg  \n",
       "85770  Michael Jackson  \n",
       "71975            Bread  \n",
       "75732     David Guetta  \n",
       "77169    Fifth Harmony  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87efd06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock       563\n",
       "Pop        286\n",
       "Hip Hop    151\n",
       "Name: Genre, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset['Genre'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66890806",
   "metadata": {},
   "source": [
    "Since the Genres are imbalanced we use this as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4304be16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Rock': 0, 'Pop': 1, 'Hip Hop': 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "possible_labels = df.Genre.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b705466",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset['label'] = subset.Genre.replace(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "996ad717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    subset.index.values,\n",
    "    subset.label.values,\n",
    "    test_size = 0.2,\n",
    "    random_state=42,\n",
    "    stratify=subset.label.values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "091edbe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([90614, 66945, 11708, 56317, 78223, 83365, 35499, 28693, 42136,\n",
       "       77595, 82675, 85607, 42509, 11460, 38540, 68823, 43948, 34780,\n",
       "       70202, 93038, 81668, 92592,  5803, 26093, 38340, 13319,  3123,\n",
       "       29371, 71975, 58258, 74168, 11603, 14949, 57364,  1029,  6540,\n",
       "       75878,  8242, 32246,  5933, 24322, 17676, 58433, 46648, 87754,\n",
       "       93211, 11051,    57,  6401,  9495, 76535, 57077, 81699, 56343,\n",
       "       94031, 26385, 20513, 53614, 71550, 52168, 75855, 13094, 32235,\n",
       "       23422, 78795,  9325, 52996, 81989, 42398, 22779,  9785,  9257,\n",
       "       78340, 39703, 71360, 21262, 42746, 72696, 89887, 46491, 70973,\n",
       "       32254, 14785, 38463, 66455, 60751, 68554, 28422,  3436, 28777,\n",
       "       64982, 18829, 89829, 51070, 25790, 50182, 26395, 87178, 38301,\n",
       "       14446, 63232, 53849, 25458, 90721, 11448,  5661, 62158,  3003,\n",
       "       42821, 46794, 27810, 64416, 42332, 91261, 49698, 83676, 13608,\n",
       "       63962, 87522, 70902, 79612, 94246, 84326, 45819, 57618, 85770,\n",
       "       39130, 84064,  5554, 82778, 61327, 54726, 18506, 22030, 63447,\n",
       "       92895, 70551, 88946, 64338,  9274, 68877, 63345, 49082, 40242,\n",
       "       32465, 17263, 20173, 92244, 62550, 45216, 30618,  6397, 19285,\n",
       "        5071, 51618, 42204, 45956, 16021, 25643, 36879, 69314, 76322,\n",
       "       31561, 26167,  6645, 29308,  5890, 72946, 90969,  4268, 64605,\n",
       "       43024, 59583, 39230, 92670,  9349,  5993,  2091, 51772, 25703,\n",
       "       61945, 66446, 15676, 54848,  1095, 38447,   165, 48252, 44233,\n",
       "       90764, 61604,  2477, 38661, 11805, 46767, 91420, 84285, 16626,\n",
       "       24130, 22170, 35664, 79873, 53685, 62222, 63810, 90582, 56957,\n",
       "       31333, 26496,   225, 58167, 60123, 15183, 90776, 86061, 79266,\n",
       "        6599, 16766, 21323, 56357, 36468, 58812, 85336, 13159, 18016,\n",
       "       38289, 50863, 72956, 48931, 90348, 11099,  8917, 48264, 93526,\n",
       "       78458,  4961,  3745,  6641, 39665, 75466, 48566, 14169, 16335,\n",
       "       62712,  9923, 31846, 86269, 26998, 40563, 22100, 14644, 20686,\n",
       "       94048,  7541, 30529, 52167, 73338, 32213, 38005, 78963, 89973,\n",
       "       18087, 33808, 83526, 51801, 29506, 85073, 93785, 39201, 33296,\n",
       "       75912, 82569, 32141, 47285, 80442, 65142, 65922, 78851, 87230,\n",
       "       27110, 77169, 22262, 30854,  6249, 37859,  1900, 36225, 90111,\n",
       "       72173, 61946,  3657, 43308, 48563, 55789, 22549,  8646, 25383,\n",
       "       85717, 43618, 42943,  1561, 46688,  7841, 49912, 67675, 78509,\n",
       "        4127, 63187,  7985, 77023, 44196, 55275, 67862, 92314,  4041,\n",
       "       49474, 29311, 92562, 70563, 82067, 75667, 78992, 32054, 48553,\n",
       "       83183, 11328, 39566, 41254, 34406,  7130, 10315, 64871, 31452,\n",
       "       68632, 71068, 61688, 67183, 69127, 90681, 47348, 68588, 20022,\n",
       "        4742, 48344, 70872, 17051, 92470, 91714, 92498, 82955, 87289,\n",
       "       92453, 93908, 67201, 55318, 37594, 35817, 46515, 14126, 58977,\n",
       "       75623, 48999, 35907, 87553, 90271, 65155, 24133, 53412, 48270,\n",
       "        8405, 75796, 20624, 43218, 79047, 24729, 69039, 77744, 83661,\n",
       "       89979,  1836, 66319, 40683, 43159, 79640, 48773, 15078, 36930,\n",
       "       71045, 53748, 12853, 44250, 91431, 56246, 93377, 57898, 25393,\n",
       "       46067, 59859, 11686, 87945,  2661, 75279, 54608, 16728, 56217,\n",
       "       38121, 84756, 83107, 14559, 15308, 19304, 78343, 26418, 74853,\n",
       "       29881,  5245,  4913, 50223, 38867, 29922, 65400, 30978, 55434,\n",
       "        8519, 60523, 62414, 73723, 64887, 92982, 23387, 18554, 22514,\n",
       "       40847, 78756, 75869,  7222, 10578, 20176, 69876, 20716, 41909,\n",
       "       75663, 94752, 72756, 85788, 28992,   698, 57178, 46100, 63878,\n",
       "       10391, 20645, 21268,  6694, 88415, 70432, 80265,  8545,  2422,\n",
       "       59677,   814, 79638, 46042, 53575, 87896, 19229, 43893, 80396,\n",
       "         657, 59892, 56201, 46567, 13272,  2507, 54959,  6404, 59976,\n",
       "        9640, 39021, 12990,  1907, 88204, 89237, 65073,  7542, 35356,\n",
       "       53190, 75780, 79178, 83732, 19559, 30464, 67747, 45512, 64189,\n",
       "       37187, 62925,  7935, 85689, 88469,   519, 94114, 71936, 52229,\n",
       "       93728, 91447, 28915, 77648,  7367, 32182, 65321, 10133, 86503,\n",
       "       58950, 75732,  2583, 90178, 46335, 76666, 33681, 14920, 72940,\n",
       "       11335, 41155,  5487, 51322, 52733, 20583, 83219, 63820, 48138,\n",
       "       42846, 29387, 29592, 22393, 31103, 39906, 93586, 84684, 34429,\n",
       "       46417, 46997, 58734, 82391, 60756, 12753, 72239, 91371, 84077,\n",
       "       77110,  9343, 89300, 72360, 28218, 44390, 16322, 46945, 50706,\n",
       "       19570, 79252, 71956, 65877, 57109, 76332, 63714, 25814, 34107,\n",
       "       57218,  3101, 42202, 63922, 74985, 79336, 61049, 16575, 38091,\n",
       "       19371, 10185, 94328, 26890, 66003, 61372, 17826, 72731, 70803,\n",
       "       51422, 50774, 16528, 69725, 11895, 39088, 19510,    39,  1464,\n",
       "       88798, 31603, 51544, 75713, 87234, 33740, 25596, 30693, 35263,\n",
       "       49507, 80640, 46135, 45029, 61313,  2678, 65488, 28494,  3257,\n",
       "       59682, 53215, 93813, 11261, 89221, 91655, 44715, 29024, 31302,\n",
       "       20359, 40541, 89029, 50816, 23353, 18423, 11853, 10899, 81794,\n",
       "       21843, 18071, 76508, 55979, 71551,  8218, 48295, 84377, 93241,\n",
       "       27616, 39246, 90155, 73893, 34353, 67402, 50936, 70281, 52287,\n",
       "        5940, 64847,  2942, 31970, 43477, 63732, 44546, 44347, 56648,\n",
       "       77854, 94100, 41996, 67497, 11756, 29029, 66779, 85467, 23168,\n",
       "       48685, 38779, 90572, 13864, 22938,  4451, 56383, 43465, 86552,\n",
       "       91457, 62173, 31509, 55479, 25450,  6508, 36454, 34310,  3293,\n",
       "       16091, 60736, 20831, 62441, 56737, 42075, 75369, 26834, 29682,\n",
       "       25125, 90027, 59584, 65062, 45446, 58418, 18508, 32627, 68277,\n",
       "       40273, 87102, 58213, 93910, 62399, 17194, 49550, 27949,  6185,\n",
       "       54383, 87825, 81024, 78315, 19591,  5406, 55944, 33465, 38500,\n",
       "        5785, 57181,  7305, 25574, 17732, 70013, 11090, 74919, 77629,\n",
       "       24526, 69059, 12837, 94717, 17046, 27241, 81565, 73436, 13407,\n",
       "       23494, 26650, 60282, 78504, 12180, 38271, 75032, 39714, 77737,\n",
       "       29928, 10066, 86245, 61944, 72098, 89563, 42627, 59184, 50861,\n",
       "       46372, 14052, 66398, 80081, 44011, 74426,  2567, 51156, 18592,\n",
       "       18513, 26805, 68040, 50273, 23108, 24077, 43520, 74026, 44153,\n",
       "        3643, 73259, 75160, 68750, 26602, 54557, 36845, 32373, 28242,\n",
       "        7457, 88909, 28790, 93745, 38060, 76270, 67360, 65535, 13973,\n",
       "       90892, 15223, 15283, 89118, 86405, 52607,   715, 13924])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0da7bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set',\n",
       " 'not_set']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['not_set']*subset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1465c615",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset=subset.drop(['ALink', 'SName', 'SLink', 'Idiom', 'Artist'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abc9b0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Genre</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75663</th>\n",
       "      <td>There is a beacon. You might not see the light...</td>\n",
       "      <td>Pop</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45819</th>\n",
       "      <td>Do it! Do it! All right!. Do it! Do it! All ri...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21810</th>\n",
       "      <td>I remember the sun beating down. Across my bac...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70872</th>\n",
       "      <td>Life is a moment in space. When the dream is g...</td>\n",
       "      <td>Pop</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84756</th>\n",
       "      <td>I have a tale to tell.. Sometimes it gets so h...</td>\n",
       "      <td>Pop</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64416</th>\n",
       "      <td>[Intro]. Snoop Dogg. Konvict Music, Akon and D...</td>\n",
       "      <td>Hip Hop</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85770</th>\n",
       "      <td>I sing of what you see and might think about. ...</td>\n",
       "      <td>Pop</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71975</th>\n",
       "      <td>I found her diary underneath a tree. and start...</td>\n",
       "      <td>Pop</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75732</th>\n",
       "      <td>I was bound, was tired. Hadn't seen a light so...</td>\n",
       "      <td>Pop</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77169</th>\n",
       "      <td>It's Gucci. Fifth Harmony. I need somebody wit...</td>\n",
       "      <td>Pop</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Lyric    Genre  label\n",
       "75663  There is a beacon. You might not see the light...      Pop      1\n",
       "45819  Do it! Do it! All right!. Do it! Do it! All ri...     Rock      0\n",
       "21810  I remember the sun beating down. Across my bac...     Rock      0\n",
       "70872  Life is a moment in space. When the dream is g...      Pop      1\n",
       "84756  I have a tale to tell.. Sometimes it gets so h...      Pop      1\n",
       "...                                                  ...      ...    ...\n",
       "64416  [Intro]. Snoop Dogg. Konvict Music, Akon and D...  Hip Hop      2\n",
       "85770  I sing of what you see and might think about. ...      Pop      1\n",
       "71975  I found her diary underneath a tree. and start...      Pop      1\n",
       "75732  I was bound, was tired. Hadn't seen a light so...      Pop      1\n",
       "77169  It's Gucci. Fifth Harmony. I need somebody wit...      Pop      1\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ad82641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Lyric</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Genre</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Hip Hop</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Pop</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Rock</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Lyric\n",
       "Genre   label data_type       \n",
       "Hip Hop 2     train        121\n",
       "              val           30\n",
       "Pop     1     train        229\n",
       "              val           57\n",
       "Rock    0     train        450\n",
       "              val          113"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding a column with 'not_set'\n",
    "subset['data_type'] = ['not_set']*subset.shape[0]\n",
    "\n",
    "#Setting the train/validation values\n",
    "subset.loc[X_train, 'data_type'] = 'train'\n",
    "subset.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "subset.groupby(['Genre', 'label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a188df0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a9588fe43445dfbecc4811dbda198e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3dd4fd841684a4b907f8f96a0667cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a436e03eef4038b15db3eab73561e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdeec18928c142329eadb19c2bea32d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ubuntu/miniconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2221: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## TOKENIZE THE DATA\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "                                          \n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    subset[subset.data_type=='train'].Lyric.values, \n",
    "    add_special_tokens=True,  # sequences will be encoded with the special tokens relative to their model.\n",
    "    return_attention_mask=True, # batching sequences together. will return the attention mask according to the specific tokenizer defined by the max_length attribute.\n",
    "    pad_to_max_length=True, # pad all the titles to certain maximum length\n",
    "    max_length=256, \n",
    "    return_tensors='pt' # return PyTorch\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    subset[subset.data_type=='val'].Lyric.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f019861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into input_ids, attention_masks and labels.\n",
    "# after we get encoded data set, we can create training data and validation data.\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(subset[subset.data_type=='train'].label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(subset[subset.data_type=='val'].label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f48a4c0",
   "metadata": {},
   "source": [
    "### BERT Pre-trained Model\n",
    "We are treating each Genre as its unique sequence, so one sequence will be classified to one of the three labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab7fdf3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544f066425144ef1819278512819deff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", # smaller pre-trained model.\n",
    "                                                      num_labels=len(label_dict), # number of output labels.\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea046a",
   "metadata": {},
   "source": [
    "### Data Loaders \n",
    "DataLoader combines a dataset and a sampler and provides an iterable over the given dataset\n",
    "RandomSampler is used for training and SequentialSampler for validation.\n",
    "Batch_size is set after the available memory. \n",
    "#### Start with batch_size=3? Increase when using cloud computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "229726e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d9854",
   "metadata": {},
   "source": [
    "# Optimizer & Scheduler\n",
    "- To construct an optimizer, we have to give it an iterable containing the parameters to optimize. Then, we can specify optimizer-specific options such as the learning rate, epsilon, etc.\n",
    "- Try with 5 epochs and iterate\n",
    "- Create a schedule with a learning rate that decreases linearly from the initial learning rate set in the optimizer to 0, after a warmup period during which it increases linearly from 0 to the initial learning rate set in the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a87c6899",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "                  \n",
    "epochs = 5\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e984e4",
   "metadata": {},
   "source": [
    "# Performance Metrics\n",
    "We will use f1 score and accuracy per class as performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a06a6d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4ef2dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ff061",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "662270d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd45165cac149189482776a418fa38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 0.0026302170937127444\n",
      "Validation loss: 0.0009898714279867153\n",
      "F1 Score (Weighted): 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 0.0012140879252740646\n",
      "Validation loss: 0.0005694024131021727\n",
      "F1 Score (Weighted): 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 0.0008274260876754348\n",
      "Validation loss: 0.00043676700312713745\n",
      "F1 Score (Weighted): 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training loss: 0.0007003494651730029\n",
      "Validation loss: 0.00040270269365369607\n",
      "F1 Score (Weighted): 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training loss: 0.0006763311243600348\n",
      "Validation loss: 0.00040270269365369607\n",
      "F1 Score (Weighted): 1.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "# In case you are running on gpu, you also need to set the model: model.to(device) - Is this set in the next step? \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals\n",
    "    \n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), f'/home/ubuntu/NLP{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df1dba",
   "metadata": {},
   "source": [
    "# Loading and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2c334ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Rock\n",
      "Accuracy: 105/105\n",
      "\n",
      "Class: Pop\n",
      "Accuracy: 61/61\n",
      "\n",
      "Class: Hip Hop\n",
      "Accuracy: 34/34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('/home/ubuntu/NLP5.model', map_location=torch.device('cuda')))\n",
    "\n",
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "accuracy_per_class(predictions, true_vals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP]",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "476.818px",
    "left": "1436.43px",
    "right": "20px",
    "top": "79.9744px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
