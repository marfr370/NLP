{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2cfe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_csv('unprocessed_lyrics.csv')\n",
    "df = df.groupby('Genre', group_keys=False).apply(lambda s: s.sample(5000, random_state=42)) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80e41079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SName</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Artist</th>\n",
       "      <th>lyric_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>Family Affair</td>\n",
       "      <td>Refrain:. Let's get it crunkupon. We gon' have...</td>\n",
       "      <td>Hip Hop</td>\n",
       "      <td>Mary J. Blige</td>\n",
       "      <td>567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3187</th>\n",
       "      <td>Ransom (ft. Lil' Wayne)</td>\n",
       "      <td>Ransom,. . Yeah,. its Drizzy Baby. you already...</td>\n",
       "      <td>Hip Hop</td>\n",
       "      <td>Drake</td>\n",
       "      <td>967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9164</th>\n",
       "      <td>Close To Me</td>\n",
       "      <td>T.O.S.. (50 Cent). Unstoppable, incredible, im...</td>\n",
       "      <td>Hip Hop</td>\n",
       "      <td>G-Unit</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2145</th>\n",
       "      <td>Zone</td>\n",
       "      <td>uhh, yea. uh uh uh. alright, well alright. . i...</td>\n",
       "      <td>Hip Hop</td>\n",
       "      <td>Drake</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6442</th>\n",
       "      <td>Why You Up In Here (feat. Ludacris, Git Fresh ...</td>\n",
       "      <td>Flo-Rida. Gucci!. Bird!. I done bought all thi...</td>\n",
       "      <td>Hip Hop</td>\n",
       "      <td>Flo Rida</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31738</th>\n",
       "      <td>What do You Need?</td>\n",
       "      <td>What do you need from me tonight?. I feel you ...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Goo Goo Dolls</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31425</th>\n",
       "      <td>Rebel Heart</td>\n",
       "      <td>(R. Stewart, J. Golub, C. Kentis, C. Rojas). I...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Rod Stewart</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35407</th>\n",
       "      <td>Before The Dawn</td>\n",
       "      <td>Meet me after dark again and I'll hold you. I ...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Evanescence</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35281</th>\n",
       "      <td>Spanish is the Loving Tongue</td>\n",
       "      <td>Broke my heart, lost my soul. Adios,mi cora so...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Bob Dylan</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38982</th>\n",
       "      <td>There Goes My Baby</td>\n",
       "      <td>There goes my baby going for a ride yeah there...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Ricky Nelson</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   SName  \\\n",
       "2157                                       Family Affair   \n",
       "3187                             Ransom (ft. Lil' Wayne)   \n",
       "9164                                         Close To Me   \n",
       "2145                                                Zone   \n",
       "6442   Why You Up In Here (feat. Ludacris, Git Fresh ...   \n",
       "...                                                  ...   \n",
       "31738                                  What do You Need?   \n",
       "31425                                        Rebel Heart   \n",
       "35407                                    Before The Dawn   \n",
       "35281                       Spanish is the Loving Tongue   \n",
       "38982                                 There Goes My Baby   \n",
       "\n",
       "                                                   Lyric    Genre  \\\n",
       "2157   Refrain:. Let's get it crunkupon. We gon' have...  Hip Hop   \n",
       "3187   Ransom,. . Yeah,. its Drizzy Baby. you already...  Hip Hop   \n",
       "9164   T.O.S.. (50 Cent). Unstoppable, incredible, im...  Hip Hop   \n",
       "2145   uhh, yea. uh uh uh. alright, well alright. . i...  Hip Hop   \n",
       "6442   Flo-Rida. Gucci!. Bird!. I done bought all thi...  Hip Hop   \n",
       "...                                                  ...      ...   \n",
       "31738  What do you need from me tonight?. I feel you ...     Rock   \n",
       "31425  (R. Stewart, J. Golub, C. Kentis, C. Rojas). I...     Rock   \n",
       "35407  Meet me after dark again and I'll hold you. I ...     Rock   \n",
       "35281  Broke my heart, lost my soul. Adios,mi cora so...     Rock   \n",
       "38982  There goes my baby going for a ride yeah there...     Rock   \n",
       "\n",
       "              Artist  lyric_length  \n",
       "2157   Mary J. Blige           567  \n",
       "3187           Drake           967  \n",
       "9164          G-Unit           557  \n",
       "2145           Drake           400  \n",
       "6442        Flo Rida           524  \n",
       "...              ...           ...  \n",
       "31738  Goo Goo Dolls           197  \n",
       "31425    Rod Stewart           412  \n",
       "35407    Evanescence           114  \n",
       "35281      Bob Dylan           126  \n",
       "38982   Ricky Nelson           113  \n",
       "\n",
       "[15000 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "780889a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['Lyric']\n",
    "y = df['Genre'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07816936",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLENGTH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b098be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df.Genre.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afdd5f53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages/keras_preprocessing/text.py:180: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 62534 unique tokens.\n",
      "Shape of data tensor: (15000, 500)\n",
      "Shape of label tensor: (15000,)\n",
      "[11509  3742 13903 ...  8628 14081  3452]\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=30000)\n",
    "tokenizer.fit_on_texts(x)\n",
    "sequences = tokenizer.texts_to_sequences(x)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=500)\n",
    "\n",
    "#labels = np_utils.to_categorical(y)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)\n",
    "data = data[indices]\n",
    "#labels = labels[indices]\n",
    "nb_validation_samples = int(0.1 * data.shape[0])\n",
    "print(nb_validation_samples)\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_test = data[-nb_validation_samples:]\n",
    "y_test = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ff60121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "857105ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xtrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9b25756f4a78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xtrain' is not defined"
     ]
    }
   ],
   "source": [
    "token = Tokenizer()\n",
    "\n",
    "token.fit_on_texts(x_train)\n",
    "seq = token.texts_to_sequences(xtrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fdef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_seq = pad_sequences(seq,maxlen=MAXLENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_test = token.texts_to_sequences(xtest)\n",
    "pad_seq_test = pad_sequences(seq_test,maxlen=MAXLENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861e4a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(token.word_index)+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4238caf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  89,    2,  471, ...,    7,   93, 1405],\n",
       "       [   0,    0,    0, ...,    6,  592,  845],\n",
       "       [   0,    0,    0, ...,    6, 6474,  304],\n",
       "       ...,\n",
       "       [  10,    7,   25, ...,  567,   46,   45],\n",
       "       [   1,   39,  124, ...,   28,    3,   95],\n",
       "       [   0,    0,    0, ...,   34,    8,   20]], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c2a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector = {}\n",
    "f = open('glove.twitter.27B.200d.txt',encoding='utf8')\n",
    "for line in tqdm(f):\n",
    "    value = line.split(' ')\n",
    "    word = value[0]\n",
    "    coef = np.array(value[1:],dtype = 'float32')\n",
    "    embedding_vector[word] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f59bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {w: i for i, w in enumerate(embeddings_index.keys(), 1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, dims))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector[:dims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88cf5d71",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'voc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5b3d2993cf07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0membedding_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'voc' is not defined"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 200\n",
    "embedding_matrix = np.zeros((vocab_size,200))\n",
    "for word,i in tqdm(token.word_index.items()):\n",
    "    embedding_value = embedding_vector.get(word)\n",
    "    if embedding_value is not None:\n",
    "        embedding_matrix[i] = embedding_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def texts_to_sequences(texts, word_index):\n",
    "    for text in texts:\n",
    "        tokens = text_to_word_sequence(text)\n",
    "        yield [word_index.get(w) for w in tokens if w in word_index]\n",
    "\n",
    "sequence = texts_to_sequences(['Test sentence'], word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd71eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad696312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df43bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(1024 activation=\"relu\")(embedded_sequences)\n",
    "pred = Dense(3 activation=\"sigmoid\")(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e4ed9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13500, 500)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce3472bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "ytrain_encode = np_utils.to_categorical(ytrain)\n",
    "ytest_encode = np_utils.to_categorical(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46df5d74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_2 to have 3 dimensions, but got array with shape (13500, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d1579d01cebf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#pad_seq = pad_seq.reshape((1,13500, 3))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytrain_encode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have 3 dimensions, but got array with shape (13500, 3)"
     ]
    }
   ],
   "source": [
    "#pad_seq = pad_seq.reshape((1,13500, 3))\n",
    "\n",
    "history = model.fit(pad_seq,ytrain_encode,epochs = 2,batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3679b",
   "metadata": {},
   "source": [
    "Large diff between acc on training vs testing (might be overfit)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fddd79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(pad_seq_test, batch_size=16, verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(ytest, y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafe516e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p37)",
   "language": "python",
   "name": "conda_tensorflow_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
